package services

import (
	"bytes"
	"crypto/md5"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"io"
	"math"
	"net/http"
	"os"
	"path/filepath"
	"seventhcenturyvideogroup/backend/go-echo-sqlite/config"
	"seventhcenturyvideogroup/backend/go-echo-sqlite/models"
	"sort"
	"strings"
	"sync"
	"time"
)

// DeepSeek API ç›¸å…³ç»“æ„ä½“
type DeepSeekChatRequest struct {
	Model       string                `json:"model"`
	Messages    []DeepSeekChatMessage `json:"messages"`
	Temperature float64               `json:"temperature,omitempty"`
}

type DeepSeekChatMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type DeepSeekChatResponse struct {
	ID      string               `json:"id"`
	Object  string               `json:"object"`
	Created int64                `json:"created"`
	Model   string               `json:"model"`
	Choices []DeepSeekChatChoice `json:"choices"`
}

type DeepSeekChatChoice struct {
	Index        int                 `json:"index"`
	Message      DeepSeekChatMessage `json:"message"`
	FinishReason string              `json:"finish_reason"`
}

// Deepseek Embedding API ç›¸å…³ç»“æ„ä½“
type DeepSeekEmbeddingRequest struct {
	Model          string   `json:"model"`
	Input          []string `json:"input"`
	EncodingFormat string   `json:"encoding_format,omitempty"`
}

type DeepSeekEmbeddingResponse struct {
	Object string                  `json:"object"`
	Data   []DeepSeekEmbeddingData `json:"data"`
	Model  string                  `json:"model"`
	Usage  DeepSeekEmbeddingUsage  `json:"usage"`
}

type DeepSeekEmbeddingData struct {
	Object    string    `json:"object"`
	Index     int       `json:"index"`
	Embedding []float64 `json:"embedding"`
}

type DeepSeekEmbeddingUsage struct {
	PromptTokens int `json:"prompt_tokens"`
	TotalTokens  int `json:"total_tokens"`
}

type RAGService struct {
	apiKey          string
	httpClient      *http.Client
	apiBase         string
	model           string
	documentMutex   sync.RWMutex      // ç”¨äºä¿æŠ¤æ–‡æ¡£è®¿é—®
	lastUpdateTime  time.Time         // ä¸Šæ¬¡æ›´æ–°æ—¶é—´
	documentHashMap map[string]string // æ–‡ä»¶è·¯å¾„ -> å“ˆå¸Œæ˜ å°„
	stopChan        chan bool         // æ–‡ä»¶ç›‘æ§åœæ­¢ä¿¡å·
	isMonitoring    bool              // æ˜¯å¦æ­£åœ¨ç›‘æ§
}

func NewRAGService() *RAGService {
	apiKey := os.Getenv("DEEPSEEK_API_KEY")
	if apiKey == "" {
		fmt.Println("è­¦å‘Š: DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
	}

	apiBase := os.Getenv("DEEPSEEK_API_BASE")
	if apiBase == "" {
		apiBase = "https://api.deepseek.com"
	}

	model := os.Getenv("DEEPSEEK_EMBEDDING_MODEL")
	if model == "" {
		model = "deepseek-chat"
	}

	return &RAGService{
		apiKey:          apiKey,
		apiBase:         apiBase,
		model:           model,
		httpClient:      &http.Client{Timeout: 30 * time.Second},
		documentHashMap: make(map[string]string),
		stopChan:        make(chan bool),
		lastUpdateTime:  time.Now(),
	}
}

// LoadDocuments ä»AI-data-sourceç›®å½•åŠ è½½æ‰€æœ‰markdownæ–‡ä»¶
func (r *RAGService) LoadDocuments() error {
	// è·å–å½“å‰å·¥ä½œç›®å½•
	wd, err := os.Getwd()
	if err != nil {
		return err
	}

	// æ„å»ºAI-data-sourceè·¯å¾„ï¼Œæ”¯æŒä»ä¸åŒç›®å½•è¿è¡Œ
	var dataSourcePath string
	if strings.Contains(wd, "go-echo-sqlite") {
		// å¦‚æœåœ¨go-echo-sqliteç›®å½•ä¸‹è¿è¡Œ
		dataSourcePath = filepath.Join("..", "AI-data-source")
	} else {
		// å¦‚æœåœ¨backendç›®å½•ä¸‹è¿è¡Œ
		dataSourcePath = filepath.Join("AI-data-source")
	}

	// è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
	dataSourcePath, err = filepath.Abs(dataSourcePath)
	if err != nil {
		return err
	}

	fmt.Printf("AIæ•°æ®æºè·¯å¾„: %s\n", dataSourcePath)

	return filepath.Walk(dataSourcePath, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if filepath.Ext(path) == ".md" {
			return r.processMarkdownFile(path)
		}

		return nil
	})
}

// processMarkdownFile å¤„ç†å•ä¸ªmarkdownæ–‡ä»¶
func (r *RAGService) processMarkdownFile(filePath string) error {
	content, err := os.ReadFile(filePath)
	if err != nil {
		return err
	}

	// è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
	hash := r.calculateHash(string(content))

	// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœªä¿®æ”¹
	var existingDoc models.Document
	db := config.GetDB()
	if err := db.Where("file_path = ? AND hash = ?", filePath, hash).First(&existingDoc).Error; err == nil {
		// æ–‡ä»¶æœªä¿®æ”¹ï¼Œè·³è¿‡å¤„ç†
		return nil
	}

	// æå–æ ‡é¢˜
	title := r.extractTitle(string(content))
	if title == "" {
		title = filepath.Base(filePath)
	}

	// æå–ç±»åˆ«
	category := r.extractCategory(string(content))

	// åˆ›å»ºæˆ–æ›´æ–°æ–‡æ¡£è®°å½•
	doc := models.Document{
		Title:     title,
		Content:   string(content),
		FilePath:  filePath,
		Hash:      hash,
		Category:  category,
		UpdatedAt: time.Now(),
	}

	// åˆ é™¤æ—§çš„æ–‡æ¡£å’Œåˆ†å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
	// é¦–å…ˆåˆ é™¤ç›¸å…³çš„æ–‡æ¡£å—
	db.Where("document_id IN (SELECT id FROM documents WHERE file_path = ?)", filePath).Delete(&models.DocumentChunk{})
	// ç„¶ååˆ é™¤æ–‡æ¡£
	db.Where("file_path = ?", filePath).Delete(&models.Document{})

	// ä¿å­˜æ–°æ–‡æ¡£
	if err := db.Create(&doc).Error; err != nil {
		return err
	}

	// åˆ†å‰²æ–‡æ¡£å¹¶åˆ›å»ºåˆ†å—
	chunks := r.splitDocument(string(content))
	for i, chunk := range chunks {
		embedding, err := r.generateEmbedding(chunk)
		if err != nil {
			fmt.Printf("ç”Ÿæˆå‘é‡å¤±è´¥: %v\n", err)
			continue
		}

		embeddingJSON, _ := json.Marshal(embedding)

		docChunk := models.DocumentChunk{
			DocumentID: doc.ID,
			Content:    chunk,
			ChunkIndex: i,
			Embedding:  string(embeddingJSON),
			CreatedAt:  time.Now(),
		}

		if err := db.Create(&docChunk).Error; err != nil {
			fmt.Printf("ä¿å­˜æ–‡æ¡£å—å¤±è´¥: %v\n", err)
		}
	}

	fmt.Printf("âœ“ å·²å¤„ç†æ–‡æ¡£: %s (åˆ†å—æ•°: %d, ID: %d)\n", title, len(chunks), doc.ID)
	return nil
}

// calculateHash è®¡ç®—æ–‡ä»¶å†…å®¹çš„MD5å“ˆå¸Œ
func (r *RAGService) calculateHash(content string) string {
	hash := md5.Sum([]byte(content))
	return hex.EncodeToString(hash[:])
}

// extractTitle ä»markdownå†…å®¹ä¸­æå–æ ‡é¢˜
func (r *RAGService) extractTitle(content string) string {
	lines := strings.Split(content, "\n")

	// é¦–å…ˆæŸ¥æ‰¾front matterä¸­çš„title
	if len(lines) > 0 && strings.TrimSpace(lines[0]) == "---" {
		for i := 1; i < len(lines); i++ {
			line := strings.TrimSpace(lines[i])
			if line == "---" {
				break
			}
			if strings.HasPrefix(line, "title:") {
				title := strings.TrimSpace(strings.TrimPrefix(line, "title:"))
				return strings.Trim(title, "\"'")
			}
		}
	}

	// æŸ¥æ‰¾ç¬¬ä¸€ä¸ªH1æ ‡é¢˜
	for _, line := range lines {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "# ") {
			return strings.TrimSpace(strings.TrimPrefix(line, "#"))
		}
	}

	return ""
}

// extractCategory ä»markdownå†…å®¹ä¸­æå–ç±»åˆ«
func (r *RAGService) extractCategory(content string) string {
	lines := strings.Split(content, "\n")

	// æŸ¥æ‰¾front matterä¸­çš„clubå­—æ®µä½œä¸ºç±»åˆ«
	if len(lines) > 0 && strings.TrimSpace(lines[0]) == "---" {
		for i := 1; i < len(lines); i++ {
			line := strings.TrimSpace(lines[i])
			if line == "---" {
				break
			}
			if strings.HasPrefix(line, "club:") {
				category := strings.TrimSpace(strings.TrimPrefix(line, "club:"))
				return strings.Trim(category, "\"'")
			}
		}
	}

	// æ ¹æ®å†…å®¹åˆ¤æ–­ç±»åˆ«
	contentLower := strings.ToLower(content)
	if strings.Contains(contentLower, "mad") && strings.Contains(contentLower, "mmd") {
		return "è§†é¢‘ç»„çŸ¥è¯†åº“"
	} else if strings.Contains(contentLower, "mad") {
		return "MADåˆ›ä½œ"
	} else if strings.Contains(contentLower, "mmd") {
		return "MMDåˆ›ä½œ"
	}

	return "é€šç”¨"
}

// splitDocument å°†æ–‡æ¡£åˆ†å‰²æˆå—
func (r *RAGService) splitDocument(content string) []string {
	var chunks []string

	// æŒ‰æ ‡é¢˜åˆ†å‰²
	sections := r.splitByHeaders(content)

	for _, section := range sections {
		// å¦‚æœæ®µè½ä¸è¶…è¿‡1500å­—ç¬¦ï¼Œç›´æ¥ä½¿ç”¨
		if len(section) <= 1500 {
			if strings.TrimSpace(section) != "" {
				chunks = append(chunks, strings.TrimSpace(section))
			}
		} else {
			// å¦‚æœæ®µè½å¤ªé•¿ï¼Œå°è¯•æŒ‰å­æ ‡é¢˜è¿›ä¸€æ­¥åˆ†å‰²
			subSections := r.splitBySubHeaders(section)
			for _, subSection := range subSections {
				if len(subSection) <= 1500 {
					if strings.TrimSpace(subSection) != "" {
						chunks = append(chunks, strings.TrimSpace(subSection))
					}
				} else {
					// å¦‚æœä»ç„¶å¤ªé•¿ï¼ŒæŒ‰é•¿åº¦åˆ†å‰²ä½†ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
					subChunks := r.splitByLength(subSection, 1200)
					chunks = append(chunks, subChunks...)
				}
			}
		}
	}

	return chunks
} // splitByHeaders æŒ‰æ ‡é¢˜åˆ†å‰²æ–‡æ¡£
func (r *RAGService) splitByHeaders(content string) []string {
	lines := strings.Split(content, "\n")
	var sections []string
	var currentSection strings.Builder

	for _, line := range lines {
		trimmedLine := strings.TrimSpace(line)

		// æ£€æµ‹æ ‡é¢˜è¡Œï¼ˆä»¥#å¼€å¤´ï¼‰
		if strings.HasPrefix(trimmedLine, "#") && currentSection.Len() > 0 {
			// ä¿å­˜å½“å‰æ®µè½
			if currentSection.Len() > 50 { // åªä¿å­˜æœ‰è¶³å¤Ÿå†…å®¹çš„æ®µè½
				sections = append(sections, currentSection.String())
			}
			currentSection.Reset()
		}

		currentSection.WriteString(line + "\n")
	}

	// æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
	if currentSection.Len() > 50 {
		sections = append(sections, currentSection.String())
	}

	return sections
}

// splitBySubHeaders æŒ‰å­æ ‡é¢˜è¿›ä¸€æ­¥åˆ†å‰²å†…å®¹
func (r *RAGService) splitBySubHeaders(content string) []string {
	lines := strings.Split(content, "\n")
	var sections []string
	var currentSection strings.Builder
	var headerLevel = 0

	for i, line := range lines {
		trimmedLine := strings.TrimSpace(line)

		// æ£€æµ‹ç¬¬ä¸€ä¸ªæ ‡é¢˜çš„çº§åˆ«
		if i == 0 && strings.HasPrefix(trimmedLine, "#") {
			headerLevel = len(strings.Split(trimmedLine, "#")[0]) + 1
		}

		// æ£€æµ‹å­æ ‡é¢˜ï¼ˆæ¯”å½“å‰çº§åˆ«ä½çš„æ ‡é¢˜ï¼‰
		if strings.HasPrefix(trimmedLine, "#") && headerLevel > 0 {
			currentLevel := len(strings.Split(trimmedLine, "#")[0]) + 1
			if currentLevel > headerLevel && currentSection.Len() > 0 {
				// ä¿å­˜å½“å‰æ®µè½
				if currentSection.Len() > 50 {
					sections = append(sections, currentSection.String())
				}
				currentSection.Reset()
			}
		}

		currentSection.WriteString(line + "\n")
	}

	// æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
	if currentSection.Len() > 50 {
		sections = append(sections, currentSection.String())
	}

	// å¦‚æœæ²¡æœ‰æ‰¾åˆ°å­æ ‡é¢˜ï¼Œè¿”å›åŸå†…å®¹
	if len(sections) == 0 {
		sections = append(sections, content)
	}

	return sections
}

// splitByLength æŒ‰é•¿åº¦åˆ†å‰²æ–‡æœ¬
func (r *RAGService) splitByLength(text string, maxLength int) []string {
	var chunks []string
	sentences := strings.Split(text, "ã€‚")

	var currentChunk strings.Builder

	for _, sentence := range sentences {
		sentence = strings.TrimSpace(sentence)
		if sentence == "" {
			continue
		}

		if currentChunk.Len()+len(sentence) > maxLength && currentChunk.Len() > 0 {
			chunks = append(chunks, currentChunk.String())
			currentChunk.Reset()
		}

		if currentChunk.Len() > 0 {
			currentChunk.WriteString("ã€‚")
		}
		currentChunk.WriteString(sentence)
	}

	if currentChunk.Len() > 0 {
		chunks = append(chunks, currentChunk.String())
	}

	return chunks
}

// generateEmbedding è°ƒç”¨Deepseek APIç”Ÿæˆæ–‡æœ¬å‘é‡ï¼Œå¤±è´¥æ—¶ä½¿ç”¨æœ¬åœ°ç‰¹å¾å‘é‡
func (r *RAGService) generateEmbedding(text string) ([]float64, error) {
	// å‹ç¼©è¾“å…¥æ–‡æœ¬
	compressedText := r.compressSemanticContent(text, 100)

	// å¦‚æœAPIå¯†é’¥æœªè®¾ç½®ï¼Œä½¿ç”¨æœ¬åœ°å›é€€æœºåˆ¶
	if r.apiKey == "" {
		fmt.Println("è­¦å‘Š: Deepseek APIå¯†é’¥æœªè®¾ç½®ï¼Œä½¿ç”¨æœ¬åœ°ç‰¹å¾å‘é‡")
		return r.generateLocalEmbedding(text), nil
	}

	// æ„å»ºè¯·æ±‚
	request := DeepSeekEmbeddingRequest{
		Model:          "deepseek-chat",
		Input:          []string{compressedText},
		EncodingFormat: "float",
	}

	jsonData, err := json.Marshal(request)
	if err != nil {
		fmt.Printf("æ„å»ºEmbeddingè¯·æ±‚å¤±è´¥: %vï¼Œä½¿ç”¨æœ¬åœ°å›é€€\n", err)
		return r.generateLocalEmbedding(text), nil
	}

	// å‘é€è¯·æ±‚åˆ°Deepseek API
	url := fmt.Sprintf("%s/v1/embeddings", r.apiBase)
	req, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		fmt.Printf("åˆ›å»ºEmbeddingè¯·æ±‚å¤±è´¥: %vï¼Œä½¿ç”¨æœ¬åœ°å›é€€\n", err)
		return r.generateLocalEmbedding(text), nil
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", r.apiKey))

	resp, err := r.httpClient.Do(req)
	if err != nil {
		fmt.Printf("Embedding APIè¯·æ±‚å¤±è´¥: %vï¼Œä½¿ç”¨æœ¬åœ°å›é€€\n", err)
		return r.generateLocalEmbedding(text), nil
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		fmt.Printf("è¯»å–Embeddingå“åº”å¤±è´¥: %vï¼Œä½¿ç”¨æœ¬åœ°å›é€€\n", err)
		return r.generateLocalEmbedding(text), nil
	}

	if resp.StatusCode != http.StatusOK {
		fmt.Printf("Embedding APIè¿”å›é”™è¯¯çŠ¶æ€ç  %d: %sï¼Œä½¿ç”¨æœ¬åœ°å›é€€\n", resp.StatusCode, string(body))
		return r.generateLocalEmbedding(text), nil
	}

	// è§£æå“åº”
	var response DeepSeekEmbeddingResponse
	if err := json.Unmarshal(body, &response); err != nil {
		fmt.Printf("è§£æEmbeddingå“åº”å¤±è´¥: %vï¼Œä½¿ç”¨æœ¬åœ°å›é€€\n", err)
		return r.generateLocalEmbedding(text), nil
	}

	if len(response.Data) == 0 {
		fmt.Println("Embeddingå“åº”ä¸­æ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨æœ¬åœ°å›é€€")
		return r.generateLocalEmbedding(text), nil
	}

	return response.Data[0].Embedding, nil
}

// generateLocalEmbedding ç”Ÿæˆæœ¬åœ°ç‰¹å¾å‘é‡ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
func (r *RAGService) generateLocalEmbedding(text string) []float64 {
	// æ¸…ç†æ–‡æœ¬
	text = strings.ToLower(strings.TrimSpace(text))
	words := strings.Fields(text)

	// åˆ›å»ºå›ºå®šé•¿åº¦çš„å‘é‡ï¼ˆ1024ç»´ï¼Œä¸Deepseekä¿æŒä¸€è‡´ï¼‰
	vectorSize := 1024
	vector := make([]float64, vectorSize)

	// åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆç‰¹å¾å‘é‡
	if len(words) == 0 {
		return vector
	}

	// 1. æ–‡æœ¬é•¿åº¦ç‰¹å¾
	vector[0] = float64(len(text)) / 1000.0 // å½’ä¸€åŒ–
	vector[1] = float64(len(words)) / 100.0

	// 2. å…³é”®è¯ç‰¹å¾
	keywords := []string{
		"mad", "mmd", "è§†é¢‘", "å‰ªè¾‘", "åˆ¶ä½œ", "æ•™ç¨‹", "è½¯ä»¶", "ç‰¹æ•ˆ",
		"æ¨¡å‹", "åŠ¨ç”»", "éŸ³ä¹", "ç´ æ", "åˆ›ä½œ", "å­¦ä¹ ", "æŠ€æœ¯", "å·¥å…·",
		"ç¤¾å›¢", "æˆå‘˜", "æ´»åŠ¨", "æ¯”èµ›", "é¡¹ç›®", "åŸ¹è®­", "æŒ‡å¯¼", "é—®é¢˜",
	}

	for i, keyword := range keywords {
		if i+2 < vectorSize && strings.Contains(text, keyword) {
			vector[i+2] = 1.0
		}
	}

	// 3. å­—ç¬¦é¢‘ç‡ç‰¹å¾
	charCount := make(map[rune]int)
	for _, char := range text {
		charCount[char]++
	}

	// é€‰æ‹©ä¸€äº›å¸¸è§å­—ç¬¦ä½œä¸ºç‰¹å¾
	commonChars := []rune{'çš„', 'æ˜¯', 'å’Œ', 'åœ¨', 'æœ‰', 'ç”¨', 'è¦', 'å¯', 'ä»¥', 'ä¼š'}
	for i, char := range commonChars {
		if i+50 < vectorSize {
			if count, exists := charCount[char]; exists {
				vector[i+50] = float64(count) / float64(len(text))
			}
		}
	}

	// 4. æ–‡æœ¬ç»“æ„ç‰¹å¾
	if strings.Contains(text, "#") {
		vector[100] = 1.0 // åŒ…å«æ ‡é¢˜
	}
	if strings.Contains(text, "```") {
		vector[101] = 1.0 // åŒ…å«ä»£ç 
	}
	if strings.Contains(text, "http") {
		vector[102] = 1.0 // åŒ…å«é“¾æ¥
	}

	// 5. åŸºäºæ–‡æœ¬å†…å®¹çš„ç‰¹å¾å“ˆå¸Œ
	for i := 200; i < vectorSize; i++ {
		if i < len(text) {
			vector[i] = float64(text[i%len(text)]) / 255.0
		}
	}

	return vector
}

// SearchSimilarChunks æœç´¢ç›¸ä¼¼çš„æ–‡æ¡£å—
func (r *RAGService) SearchSimilarChunks(query string, topK int, category string) ([]models.DocumentChunkResult, error) {
	// ç”ŸæˆæŸ¥è¯¢å‘é‡
	queryEmbedding, err := r.generateEmbedding(query)
	if err != nil {
		// ç”±äºgenerateEmbeddingç°åœ¨æ€»æ˜¯æˆåŠŸçš„ï¼ˆä½¿ç”¨å›é€€æœºåˆ¶ï¼‰ï¼Œè¿™é‡Œä¸åº”è¯¥å‘ç”Ÿ
		return nil, err
	}

	// ä»æ•°æ®åº“è·å–æ‰€æœ‰æ–‡æ¡£å—
	db := config.GetDB()
	var chunks []models.DocumentChunk

	query_db := db.Preload("Document")
	if category != "" {
		query_db = query_db.Joins("JOIN documents ON document_chunks.document_id = documents.id").
			Where("documents.category = ?", category)
	}

	if err := query_db.Find(&chunks).Error; err != nil {
		return nil, err
	}

	// è®¡ç®—ç›¸ä¼¼åº¦
	var results []models.DocumentChunkResult

	for _, chunk := range chunks {
		var chunkEmbedding []float64
		if err := json.Unmarshal([]byte(chunk.Embedding), &chunkEmbedding); err != nil {
			// å¦‚æœæ—§çš„å‘é‡æ— æ³•è§£æï¼Œé‡æ–°ç”Ÿæˆ
			newEmbedding, _ := r.generateEmbedding(chunk.Content)
			chunkEmbedding = newEmbedding
		}

		similarity := r.cosineSimilarity(queryEmbedding, chunkEmbedding)

		results = append(results, models.DocumentChunkResult{
			ChunkID:    chunk.ID,
			DocumentID: chunk.DocumentID,
			Title:      chunk.Document.Title,
			Content:    chunk.Content,
			Similarity: similarity,
			Category:   chunk.Document.Category,
		})
	}

	// æŒ‰ç›¸ä¼¼åº¦æ’åº
	sort.Slice(results, func(i, j int) bool {
		return results[i].Similarity > results[j].Similarity
	})

	// è¿”å›å‰topKä¸ªç»“æœ
	if topK > len(results) {
		topK = len(results)
	}

	return results[:topK], nil
}

// cosineSimilarity è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
func (r *RAGService) cosineSimilarity(a, b []float64) float64 {
	if len(a) != len(b) {
		return 0
	}

	var dotProduct, normA, normB float64

	for i := range a {
		dotProduct += a[i] * b[i]
		normA += a[i] * a[i]
		normB += b[i] * b[i]
	}

	if normA == 0 || normB == 0 {
		return 0
	}

	return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
}

// compressSemanticContent å¯¹æ–‡æœ¬è¿›è¡Œè¯­ä¹‰å‹ç¼©ï¼Œä¿ç•™å…³é”®ä¿¡æ¯
func (r *RAGService) compressSemanticContent(text string, maxLength int) string {
	text = strings.TrimSpace(text)

	if len(text) <= maxLength {
		return text
	}

	// å…³é”®è¯åˆ—è¡¨ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
	keywords := []string{
		"mad", "mmd", "è§†é¢‘", "å‰ªè¾‘", "åˆ¶ä½œ", "æ•™ç¨‹", "è½¯ä»¶", "ç‰¹æ•ˆ",
		"æ¨¡å‹", "åŠ¨ç”»", "éŸ³ä¹", "ç´ æ", "åˆ›ä½œ", "å­¦ä¹ ", "æŠ€æœ¯", "å·¥å…·",
		"ç¤¾å›¢", "æˆå‘˜", "æ´»åŠ¨", "æ¯”èµ›", "é¡¹ç›®", "åŸ¹è®­", "é—®é¢˜", "è§£å†³",
		"æ–¹æ³•", "æ­¥éª¤", "æŒ‡å—", "æ¨è", "å»ºè®®", "å¿…è¦", "é‡è¦", "å…³é”®",
	}

	// æå–å¥å­
	sentences := strings.Split(text, "ã€‚")
	var importantSentences []string

	for _, sentence := range sentences {
		sentence = strings.TrimSpace(sentence)
		if sentence == "" {
			continue
		}

		// æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
		sentenceLower := strings.ToLower(sentence)
		hasKeyword := false
		for _, keyword := range keywords {
			if strings.Contains(sentenceLower, keyword) {
				hasKeyword = true
				break
			}
		}

		// ä¼˜å…ˆä¿ç•™åŒ…å«å…³é”®è¯çš„å¥å­
		if hasKeyword {
			importantSentences = append(importantSentences, sentence)
		}
	}

	// å¦‚æœå…³é”®å¥å­è¶³å¤Ÿï¼Œä½¿ç”¨å…³é”®å¥å­
	var compressed string
	if len(importantSentences) > 0 {
		compressed = strings.Join(importantSentences, "ã€‚")
	} else {
		// å¦åˆ™ä½¿ç”¨ç¬¬ä¸€å¥æˆ–å‰å‡ ä¸ªå•è¯
		if len(sentences) > 0 {
			compressed = sentences[0]
		}
	}

	// å¦‚æœå‹ç¼©åçš„æ–‡æœ¬ä»ç„¶è¿‡é•¿ï¼Œè¿›ä¸€æ­¥æˆªæ–­
	if len(compressed) > maxLength {
		compressed = compressed[:maxLength] + "..."
	}

	return compressed
}

// compressOutput å¯¹è¾“å‡ºç»“æœè¿›è¡Œè¯­ä¹‰å‹ç¼©ï¼Œä¿ç•™å…³é”®å†…å®¹
func (r *RAGService) compressOutput(output string, maxLength int) string {
	output = strings.TrimSpace(output)

	if len(output) <= maxLength {
		return output
	}

	// ä¼˜å…ˆä¿ç•™ä»¥ä¸‹å†…å®¹ï¼š
	// 1. æ ‡é¢˜å’Œå…³é”®ä¿¡æ¯
	// 2. æ¨èå’Œå»ºè®®
	// 3. æ­¥éª¤è¯´æ˜

	lines := strings.Split(output, "\n")
	var importantLines []string

	importanceKeywords := []string{
		"æ­¥éª¤", "å»ºè®®", "æ¨è", "è¦ç‚¹", "æ³¨æ„", "é‡è¦", "å¿…é¡»", "å…³é”®",
		"è§£å†³", "æ–¹æ³•", "è§£ç­”", "ç­”æ¡ˆ", "ç»“è®º", "æ€»ç»“", "ã€", "---",
	}

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if line == "" {
			continue
		}

		// æ£€æŸ¥æ˜¯å¦æ˜¯é‡è¦è¡Œ
		isImportant := false
		lineLower := strings.ToLower(line)

		for _, keyword := range importanceKeywords {
			if strings.Contains(lineLower, strings.ToLower(keyword)) {
				isImportant = true
				break
			}
		}

		// ä¿ç•™æ ‡é¢˜è¡Œï¼ˆä»¥# æˆ– ## å¼€å¤´çš„è¡Œï¼‰
		if strings.HasPrefix(line, "#") {
			isImportant = true
		}

		if isImportant {
			importantLines = append(importantLines, line)
		}
	}

	// ç»„åˆé‡è¦è¡Œ
	var compressed string
	if len(importantLines) > 0 {
		compressed = strings.Join(importantLines, "\n")
	} else {
		// å¦‚æœæ²¡æœ‰é‡è¦è¡Œï¼Œä½¿ç”¨å‰å‡ è¡Œ
		for i := 0; i < len(lines) && i < 5; i++ {
			if strings.TrimSpace(lines[i]) != "" {
				importantLines = append(importantLines, lines[i])
			}
		}
		compressed = strings.Join(importantLines, "\n")
	}

	// æœ€ç»ˆæˆªæ–­åˆ°æœ€å¤§é•¿åº¦
	if len(compressed) > maxLength {
		// å°è¯•åœ¨å¥å·å¤„æˆªæ–­
		truncated := compressed[:maxLength]
		lastDot := strings.LastIndex(truncated, "ã€‚")
		if lastDot > maxLength/2 {
			compressed = compressed[:lastDot] + "ã€‚"
		} else {
			compressed = truncated + "..."
		}
	}

	return compressed
}

// RefreshDocuments æ‰‹åŠ¨åˆ·æ–°çŸ¥è¯†åº“ï¼ˆçƒ­æ›´æ–°ï¼‰
func (r *RAGService) RefreshDocuments() error {
	fmt.Println("\n========== å¼€å§‹çŸ¥è¯†åº“çƒ­æ›´æ–° ==========")

	startTime := time.Now()

	// è·å–å½“å‰å·¥ä½œç›®å½•
	wd, err := os.Getwd()
	if err != nil {
		return err
	}

	// æ„å»ºAI-data-sourceè·¯å¾„
	var dataSourcePath string
	if strings.Contains(wd, "go-echo-sqlite") {
		dataSourcePath = filepath.Join("..", "AI-data-source")
	} else {
		dataSourcePath = filepath.Join("AI-data-source")
	}

	// è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
	dataSourcePath, err = filepath.Abs(dataSourcePath)
	if err != nil {
		return err
	}

	fmt.Printf("æ­£åœ¨æ‰«æ: %s\n", dataSourcePath)

	// éå†ç›®å½•ï¼Œæ£€æŸ¥æ–‡ä»¶å˜åŒ–
	var updatedCount = 0
	err = filepath.Walk(dataSourcePath, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if filepath.Ext(path) == ".md" {
			// è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
			content, err := os.ReadFile(path)
			if err != nil {
				return err
			}

			hash := r.calculateHash(string(content))
			oldHash, exists := r.documentHashMap[path]

			// å¦‚æœæ–‡ä»¶æ˜¯æ–°çš„æˆ–å·²ä¿®æ”¹ï¼Œå¤„ç†å®ƒ
			if !exists || oldHash != hash {
				fmt.Printf("  ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–: %s\n", filepath.Base(path))
				if err := r.processMarkdownFile(path); err != nil {
					fmt.Printf("  âœ— å¤„ç†å¤±è´¥: %v\n", err)
					return err
				}
				r.documentHashMap[path] = hash
				updatedCount++
			}
		}

		return nil
	})

	if err != nil {
		fmt.Printf("âœ— çŸ¥è¯†åº“çƒ­æ›´æ–°å¤±è´¥: %v\n", err)
		return err
	}

	r.lastUpdateTime = time.Now()

	fmt.Printf("âœ“ çŸ¥è¯†åº“çƒ­æ›´æ–°å®Œæˆ (æ›´æ–°: %d ä¸ªæ–‡ä»¶, è€—æ—¶: %.2fs)\n", updatedCount, time.Since(startTime).Seconds())
	fmt.Println("==========================================\n")

	return nil
}

// GetUpdateStatus è·å–çŸ¥è¯†åº“æ›´æ–°çŠ¶æ€
func (r *RAGService) GetUpdateStatus() map[string]interface{} {
	db := config.GetDB()

	var docCount int64
	db.Model(&models.Document{}).Count(&docCount)

	var chunkCount int64
	db.Model(&models.DocumentChunk{}).Count(&chunkCount)

	return map[string]interface{}{
		"last_update_time": r.lastUpdateTime,
		"documents_count":  docCount,
		"chunks_count":     chunkCount,
		"is_monitoring":    r.isMonitoring,
	}
}

// SyncMembersToMarkdown å°†æ•°æ®åº“ä¸­çš„æˆå‘˜ä¿¡æ¯åŒæ­¥åˆ°markdownæ–‡ä»¶
func (r *RAGService) SyncMembersToMarkdown() error {
	db := config.GetDB()

	// è·å–æ‰€æœ‰æˆå‘˜ä¿¡æ¯
	var members []models.ClubMember
	if err := db.Find(&members).Error; err != nil {
		return fmt.Errorf("è·å–æˆå‘˜ä¿¡æ¯å¤±è´¥: %v", err)
	}

	// ç”Ÿæˆmarkdownå†…å®¹
	var content strings.Builder
	content.WriteString("---\n")
	content.WriteString("title: æŸ’ä¸–çºªè§†é¢‘ç»„æˆå‘˜ä¿¡æ¯\n")
	content.WriteString("role: ç¤¾å›¢æˆå‘˜ä¿¡æ¯åº“\n")
	content.WriteString("club: æŸ’ä¸–çºªè§†é¢‘ç»„\n")
	content.WriteString("language: zh-CN\n")
	content.WriteString("last_updated: " + time.Now().Format("2006-01-02 15:04:05") + "\n")
	content.WriteString("---\n\n")

	content.WriteString("# æŸ’ä¸–çºªè§†é¢‘ç»„æˆå‘˜ä¿¡æ¯åº“\n\n")
	content.WriteString("æœ¬æ–‡æ¡£è®°å½•äº†æŸ’ä¸–çºªè§†é¢‘ç»„æ‰€æœ‰æ´»è·ƒæˆå‘˜çš„åŸºæœ¬ä¿¡æ¯ï¼Œç”¨äºAIåŠ©æ‰‹å¿«é€Ÿäº†è§£æˆå‘˜èƒŒæ™¯ã€‚\n\n")

	content.WriteString("## æˆå‘˜æ€»æ•°\n\n")
	content.WriteString(fmt.Sprintf("- æ€»è®¡: %d åæˆå‘˜\n", len(members)))
	content.WriteString("- æ›´æ–°æ—¶é—´: " + time.Now().Format("2006-01-02 15:04:05") + "\n\n")

	content.WriteString("## æˆå‘˜è¯¦ç»†ä¿¡æ¯\n\n")

	// æŒ‰å…¥ç¤¾æ—¶é—´æ’åº
	for i, member := range members {
		content.WriteString(fmt.Sprintf("### %d. %s\n\n", i+1, member.CN))
		content.WriteString(fmt.Sprintf("**æ€§åˆ«**: %s\n\n", member.Sex))
		content.WriteString(fmt.Sprintf("**å¹´çº§**: %s\n\n", member.Year))
		content.WriteString(fmt.Sprintf("**æ–¹å‘**: %s\n\n", member.Direction))
		content.WriteString(fmt.Sprintf("**èŒä½**: %s\n\n", member.Position))
		content.WriteString(fmt.Sprintf("**çŠ¶æ€**: %s\n\n", member.Status))

		if member.Remark != "" {
			content.WriteString(fmt.Sprintf("**å¤‡æ³¨**: %s\n\n", member.Remark))
		}

		content.WriteString("---\n\n")
	}

	// è·å–æ•°æ®æºè·¯å¾„
	wd, err := os.Getwd()
	if err != nil {
		return err
	}

	var dataSourcePath string
	if strings.Contains(wd, "go-echo-sqlite") {
		dataSourcePath = filepath.Join("..", "AI-data-source", "ç¤¾å›¢æˆå‘˜ä¿¡æ¯.md")
	} else {
		dataSourcePath = filepath.Join("AI-data-source", "ç¤¾å›¢æˆå‘˜ä¿¡æ¯.md")
	}

	dataSourcePath, err = filepath.Abs(dataSourcePath)
	if err != nil {
		return err
	}

	// å†™å…¥æ–‡ä»¶
	if err := os.WriteFile(dataSourcePath, []byte(content.String()), 0644); err != nil {
		return fmt.Errorf("å†™å…¥æˆå‘˜ä¿¡æ¯æ–‡ä»¶å¤±è´¥: %v", err)
	}

	fmt.Printf("âœ“ å·²åŒæ­¥æˆå‘˜ä¿¡æ¯åˆ°: %s (%d ä¸ªæˆå‘˜)\n", dataSourcePath, len(members))

	// è§¦å‘çŸ¥è¯†åº“é‡æ–°åŠ è½½
	if err := r.RefreshDocuments(); err != nil {
		fmt.Printf("âš  æˆå‘˜ä¿¡æ¯å·²æ›´æ–°ï¼Œä½†çŸ¥è¯†åº“é‡æ–°åŠ è½½å¤±è´¥: %v\n", err)
		// ä¸è¿”å›é”™è¯¯ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»æˆåŠŸå†™å…¥
	}

	return nil
}

// EnhanceQuery ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å¢å¼ºæŸ¥è¯¢
func (r *RAGService) EnhanceQuery(originalQuery string, relevantChunks []models.DocumentChunkResult) string {
	if len(relevantChunks) == 0 {
		return originalQuery
	}

	var contextBuilder strings.Builder
	contextBuilder.WriteString("æ ¹æ®ä»¥ä¸‹ç›¸å…³çŸ¥è¯†å›ç­”é—®é¢˜ï¼š\n\n")

	for i, chunk := range relevantChunks {
		contextBuilder.WriteString(fmt.Sprintf("ã€ç›¸å…³èµ„æ–™%d - %sã€‘\n", i+1, chunk.Title))
		contextBuilder.WriteString(chunk.Content)
		contextBuilder.WriteString("\n\n")
	}

	contextBuilder.WriteString("ç”¨æˆ·é—®é¢˜ï¼š" + originalQuery + "\n\n")
	contextBuilder.WriteString("è¯·åŸºäºä¸Šè¿°ç›¸å…³èµ„æ–™ï¼Œä»¥è§†é¢‘ç»„AIå°åŠ©ç†çš„èº«ä»½å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å›ç­”è¦ï¼š\n")
	contextBuilder.WriteString("1. ä¸“ä¸šä¸”æ¸©æš–ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡\n")
	contextBuilder.WriteString("2. ç»“åˆç›¸å…³èµ„æ–™ç»™å‡ºå…·ä½“å»ºè®®\n")
	contextBuilder.WriteString("3. å¦‚æœæ˜¯MADæˆ–MMDç›¸å…³é—®é¢˜ï¼Œè¦æ˜ç¡®åŒºåˆ†å¹¶ä½¿ç”¨å¯¹åº”æ¨¡å—ä¿¡æ¯\n")
	contextBuilder.WriteString("4. æä¾›å®ç”¨çš„æ­¥éª¤æˆ–å»ºè®®\n")
	contextBuilder.WriteString("5. é¼“åŠ±ç”¨æˆ·ç»§ç»­å­¦ä¹ å’Œåˆ›ä½œ\n\n")

	// æ·»åŠ è¯­ä¹‰å‹ç¼©æç¤ºè¯
	contextBuilder.WriteString("ã€è¾“å‡ºä¼˜åŒ–è¦æ±‚ - è¯­ä¹‰å‹ç¼©ã€‘\n")
	contextBuilder.WriteString("è¯·åœ¨å›ç­”æ—¶è¿›è¡Œé€‚åº¦çš„è¯­ä¹‰å‹ç¼©ä»¥ä¼˜åŒ–è¾“å‡ºé•¿åº¦ï¼š\n")
	contextBuilder.WriteString("- ç§»é™¤å†—ä½™å’Œé‡å¤è¡¨è¿°ï¼Œä½†ä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯\n")
	contextBuilder.WriteString("- åˆå¹¶ç›¸ä¼¼çš„æ­¥éª¤æˆ–å»ºè®®ï¼Œä½¿ç”¨æ›´ç®€æ´çš„è¡¨è¾¾\n")
	contextBuilder.WriteString("- ä½¿ç”¨åˆ—è¡¨ã€åºå·ã€ä»£ç å—ç­‰æ ¼å¼æé«˜å¯è¯»æ€§\n")
	contextBuilder.WriteString("- å¿…é¡»ä¿ç•™æ‰€æœ‰é‡è¦è­¦å‘Šã€ç‰ˆæƒæé†’å’Œæ³¨æ„äº‹é¡¹\n")
	contextBuilder.WriteString("- ç›®æ ‡ï¼šå°†å†…å®¹å‹ç¼©åˆ°åŸæ–‡æœ¬çš„ 70-85% é•¿åº¦\n")

	return contextBuilder.String()
}
